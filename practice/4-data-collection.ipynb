{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data management\n",
    "\n",
    "### Scraping the Internet to Collect Data\n",
    "\n",
    "## [Malka Guillot](https://malkaguillot.github.io/)\n",
    "\n",
    "### HEC Liège | [ECON2306]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Outline\n",
    "1. Introduction\n",
    "2. HTML: scraping and parsing\n",
    "    - Gathering (unstructured) web data and transforming it into structured data (“web scraping”).\n",
    "3. Web APIs\n",
    "    - Accessing data on the web: APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "**Publication of crawling papers by year**\n",
    "\n",
    "<center> \n",
    "    \n",
    "<div class=\"r-stack\"><img src=\"images/publication_crawling_papers_by_year.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n",
    "\n",
    "*Source*: Claussen, Jörg and Peukert, Christian, **[Obtaining Data from the Internet: A Guide to Data Crawling in Management Research](https://ssrn.com/abstract=3403799)** (June 2019). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Examples in Economics\n",
    "1. Davis and Dingell (2016): [\"How Segregated Is Urban Consumption\"](http://www.jdingel.com/research/DavisDingelMonrasMorales.pdf) \n",
    "   - use Yelp to look racial segregation in consumption (do different races consume different things?)\n",
    "2. Cavallo and Rigobon (2015): [\"The Billion Prices Project: Using Online Prices for Measurement and Research.\"](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.30.2.151)\n",
    "   - collects prices from online retailers to look at macro price changing issues; also \n",
    "   - see also Cavallo (2015) “Scraped Data and Sticky Prices”\n",
    "3. Halket and Pignatti (2015): \n",
    "   - scrape Craigslist to better understand US rental market\n",
    "4. Many papers on eBay, some on Alibaba\n",
    "5. Edleman, B. [\"Using Internet Data for Economic Research.\"](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.26.2.189), (JEP 2012): \n",
    "   - useful discussion of many issues\n",
    "\n",
    "#### Coding resources\n",
    "- [Python's requests & Beautiful Soup libraries](https://blog.hartleybrody.com/web-scraping-cheat-sheet/) (for web scraping & APIs)\n",
    "- Ryan Mitchell, [Web Scraping with Python](https://learning.oreilly.com/library/view/web-scraping-with/9781491985564/), O'Reilly Media, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Example of data\n",
    "\n",
    "- **Online markets**: \n",
    "    - housing, job, goods\n",
    "    \n",
    "- **Social media**: \n",
    "    - Twitter, Facebook, Wechat, newspaper text\n",
    "    \n",
    "- **Historical data** using the internet Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### What is webscraping ?\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/screenscraping.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n",
    "\n",
    "Source: [SICSS](https://compsocialscience.github.io) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### What is Web Scraping?\n",
    "- Process of **gathering information** from the Internet\n",
    "    - structure or unstructured info\n",
    "- Involves **automation** \n",
    "\n",
    "#### Challenges of Web Scraping\n",
    "- **Variety**. Every website is different.\n",
    "- **Durability**. Websites constantly change\n",
    "\n",
    "\n",
    "#### Points to keep in mind\n",
    "- It may or may not be legal\n",
    "    - Look at websites’ terms of service and robots.txt files\n",
    "- Webscraping is tedious and frustrating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Getting started: Things to consider before you begin\n",
    "- **What**  do you want ?\n",
    "    - Is the website only online for a limited time? \n",
    "    - Do you want an original snapshot as a backup? \n",
    "    - Is it more convenient to filter your data offline?\n",
    "    \n",
    "- **How** do you want to proceed? \n",
    "    - What scraping approach (depends on the website)?\n",
    "    - Which `python package` is needed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Best practices\n",
    "\n",
    "#### 1. Check out the data are already available \n",
    "\n",
    "- Send an **email** to try to get the data directly\n",
    "- Search if somebody has already **faced the same or a similar problem**.\n",
    "- Does the site or service provide an **API** that you can access directly?\n",
    "    - An API or Application programming interface helps you get data you need via a simple computer program!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2. Be gentle\n",
    "- If possible, you can scrape during off-peak hours \n",
    "- Limit the number of parallel / concurrent requests\n",
    "- Spread the requests across multiple IP's \n",
    "- Add delays to successive requests  \n",
    "- Avoid unnecessary requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### 3. Respect `Robots.txt`\n",
    "\n",
    "The `robot.txt` = a text file the website administrators create to instruct web scrapers on how to crawl pages on their website. \n",
    "\n",
    "$\\rightarrow$ lays out the rules for acceptable behavior \n",
    "    - which web pages can and can't be scraped, \n",
    "    - which user agents are not allowed\n",
    "    - how fast you can do it, \n",
    "    - how frequently you can do it\n",
    "\n",
    "I'd also recommend you read the terms of service of the website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 4. Don't follow the same crawling pattern.\n",
    "\n",
    "Even though human users and bots consume data from a web page, there are some differences:  \n",
    "\n",
    "- Real Humans are slow & unpredictable, \n",
    "- Bots are fast but predictable. \n",
    "\n",
    "$\\Rightarrow$ used by anti-scraping technologies to block web scraping.\n",
    "\n",
    "*Solution*: incorporate some random actions that confuse the anti-scraping technology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Most important python library for data collection\n",
    "- Standard: \n",
    "    - `Requests`\n",
    "    - `Beautiful Soup`\n",
    "- More advanced\n",
    "    - `Scrapy` [documentation](https://docs.scrapy.org/en/latest/)\n",
    "    - `Selenium` \n",
    "\n",
    "$+$ installing the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages + set options\n",
    "from IPython.display import display\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None # Display all columns of a dataframe\n",
    "pd.options.display.max_rows = 700\n",
    "\n",
    "from pprint import pprint\n",
    "import re  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data communication for the World Wide Web \n",
    "\n",
    "- `HTTP protocol`= way of communication between the client (browser) and the web server \n",
    "    - no encryption $\\rightarrow$ not safe\n",
    "- `HTTPS protocol`= S for secured\n",
    "\n",
    "$\\Rightarrow $ works by doing `Requests` and `Responses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " ### Static vs. dynamic websites \n",
    "\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://about.gitlab.com/images/blogimages/ssg-gitlab-pages-series/part-1-dynamic-x-static-server.png\" style=\"height: 450px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- **Static Websites**: the server that hosts the site sends back HTML documents that already contain all the data you’ll get to see as a user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Request and Response\n",
    "\n",
    "All interactions between a client and a web server are split into a request and a response:\n",
    "\n",
    "- `Requests` contain relevant data regarding your request call:\n",
    "    - base URL \n",
    "        - \\[ More on this for **API**: the *endpoint*, the *method* used, the *headers*, and so on.\\]\n",
    "- `Responses` contain relevant data returned by the server:\n",
    "    - the data or content, the status code, and the headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `get` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "url='https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Request`'s attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = response.request\n",
    "print('request: ',request)\n",
    "print('-----')\n",
    "print('url: ',request.url)\n",
    "print('-----')  \n",
    "print('path_url: ', request.path_url)\n",
    "print('-----')\n",
    "print('Method: ', request.method)\n",
    "print('-----')\n",
    "print('Method: ', request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `Response`'s attributes\n",
    "\n",
    "- `.text` returns the response contents in Unicode format\n",
    "- `.content` returns the response contents in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('', response)\n",
    "print('-----')\n",
    "print('Text:', response.text[:50]) # Only the first 50 characters\n",
    "print('-----')\n",
    "print('Status_code:', response.status_code)\n",
    "print('-----')\n",
    "print('hHeaders:', response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Status Codes\n",
    "\n",
    "**Important information**: \n",
    "if your request was successful, if it’s missing data, if it’s missing credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>    \n",
    "<div class=\"r-stack\"><img src=\"https://bitmaskers.in/content/images/2023/10/1-1.jpg\" style=\"height: 400px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping & Parsing in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### STEPS:\n",
    "1. Inspect Your Data Source\n",
    "1.  Scrape HTML Content From a Page\n",
    "1.  Parse HTML Code With Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: Inspect Your Data Source\n",
    "\n",
    "#### Explore the Website \n",
    "\n",
    "**Objective**: understanding its underlying structure\n",
    "\n",
    "*We will scrape the list of current members of the U.S. Congress*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Website example for today\n",
    "\n",
    " \n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/ballotpedia.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n",
    "\n",
    "Source: [ballotpedia website](https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Understanding URLs\n",
    "- **Base URL**: https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress\n",
    "- More complex URL with query parameter https://ballotpedia.org/wiki/index.php?search=jerry\n",
    "    - query parameter=`p?search=jerry`\n",
    "    - can be used to crawl websites if you have a list of queries that you want to loop over (e.g. dates, localities...)\n",
    "    - query structure:\n",
    "        - *Start*: `?`\n",
    "        - *Information*: pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (key=value). \n",
    "        - *Separator*: `&` -> if multiple query parameters \n",
    "        \n",
    "Other example of URL: https://opendata.swiss/en/dataset?political_level=commune&q=health. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "<p> Try to change the search and selection parameters and observe how that affects the  <a href= \"https://opendata.swiss/en/dataset?political_level=commune&q=health\">URL</a> . \n",
    "<p>   Next, try to change the values directly in your URL. See what happens when you paste the following URL into your browser’s address bar:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Conclusion**: When you explore URLs, you can get information on how to retrieve data from the website’s server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inspect the site: Using Developer Tools\n",
    "We use the `inspect` function (right click) to access the underlying HTML interactively. \n",
    "\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/ballotpedia_inspect.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Developer tools\n",
    "- Developer tools can help you understand the structure of a website\n",
    "- I use it in chrome or firefox, but exists for most browsers\n",
    "- Interactively explore the source html & the webpage\n",
    "\n",
    "**`html` is great but intricated $\\Rightarrow$ sublimed by `beautifulsoup`** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Step 2: Scrape HTML Content From a Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress'\n",
    "response = requests.get(url)\n",
    "html=response.text\n",
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "html looks messy.\n",
    "\n",
    "Using the `prettify()` function from `BeautifulSoup` helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Parse raw HTML\n",
    "from bs4 import BeautifulSoup # package for parsing HTML\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse html of web page\n",
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Step 3: Parse HTML Code With Beautiful Soup\n",
    "\n",
    "**Objectif**: extract url of senators from the webpage to build a list of url that will be used for scraping info on senators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Find Elements by ID using `find`\n",
    "\n",
    "In an HTML web page, every element can have an id attribute assigned. \n",
    "\n",
    "Can be used to directly access the element. \n",
    "\n",
    "**Syntax**: \n",
    "- `soup.find(id='the-id-of-what-you-are-looking for')`\n",
    "- Output: some text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"images/ballotpedia-senate.png\" style=\"height: 420px;\" > </div>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "balance=soup.find(id='Leadership_and_partisan_balance')\n",
    "print(balance.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4> Your turn</h4>\n",
    "<p> Extract the page title \"List of current members of the U.S. Congress\" using the find method\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"firstHeading\").prettify() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "officeholder_table=soup.find(id='firstHeading')\n",
    "print(officeholder_table.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the table entitled *List of current U.S. Senate members*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "officeholder_table=soup.find(id='officeholder-table')\n",
    "print(officeholder_table.prettify()[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Find Elements by HTML Class Name (using `find_all`)\n",
    "Because the result is not unique `find_all` instead of `find`. \n",
    "\n",
    "Lets' rely on the html structure to find the row of the table\n",
    "\n",
    "**Syntax**: \n",
    "- `soup.find_all(id='the-id-of-what-you-are-looking for')`\n",
    "- Output: a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "thead= officeholder_table.find('thead')\n",
    "thead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rows=officeholder_table.find_all('tr')\n",
    "len(rows) # consistent: 100 members + headline + 2 delegations from puerto rico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Find Elements by CSS Class using `select_one()` or  `find_all()`\n",
    "\n",
    "In an HTML web page, some tag can contain a \"class\" attribute. \n",
    "\n",
    "It can be useful if you want to get information from a tag which has no id attribute:\n",
    "\n",
    "`<table class=\"bptable gray sortable jquery-tablesorter\" id=\"officeholder-table\" style=\"width:auto; border-bottom:1px solid #bcbcbc;\">`\n",
    "\n",
    "Note that this table tag has **4 classes**: \n",
    "- bptable, \n",
    "- gray, \n",
    "- sortable \n",
    "- jquery-tablesorter. \n",
    "\n",
    "In this case you can use any of the three first class name to get the `officeholder_table`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### First syntax:  `select_one(tag_name.class_name)` \n",
    "You can select multiple elements with `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "officeholder_table = soup.select_one('table.bptable')\n",
    "print(officeholder_table.prettify()[:300]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Second syntax: `find(tag_name, {'attribute_name' : 'attribute_value'})`\n",
    "You can select multiple elements with `find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "officeholder_table = soup.find('table', {'class' : 'bptable'})\n",
    "print(officeholder_table.prettify()[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let's try to get the `url` for one example row:\n",
    "We will use the `html tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row=rows[1]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tds=row.find_all('td')\n",
    "tds[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= tds[1].find_all('a') \n",
    "print('--')\n",
    "print('a list:', url)\n",
    "print('--')\n",
    "print('its unique element', url[0])\n",
    "print('--')\n",
    "print('url wanted', url[0]['href'] )\n",
    "print('--')\n",
    "print('Text content', url[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4> Your turn</h4>\n",
    "<p> Use the code for 1 row in order to build a loop that gives a list of all of the wanted url. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list_url=[]\n",
    "for row in rows[1:]: \n",
    "    tds=row.find_all('td')\n",
    "    url = tds[1].find(\"a\")['href']\n",
    "    list_url.append(url)\n",
    "len(list_url)\n",
    "print(list_url[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h4> Your turn</h4>\n",
    "<p> Get the table containing the \"List of current U.S. House members\". \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "officeholder_tables=soup.find_all(id='officeholder-table')\n",
    "print(\"There are {} tables\".format(len(officeholder_tables)))\n",
    "\n",
    "house_table=officeholder_tables[1] # the second table\n",
    "print(\"------ looking at the content of the second one ------\")\n",
    "print(house_table.prettify()[450:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Your First Scraper\n",
    "\n",
    "Then, the same logic can be implemented to get the info from the senators' page (e.g. https://ballotpedia.org/Jerry_Moran). The following code extracts info from the first 10 url from the list scraped above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Strategy\n",
    "\n",
    "- Loop of a list of url (`list_url`)\n",
    "    - get the soup for the `url` $\\rightarrow$ `get_the_soup()`\n",
    "        - Extract information from the `soup`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#1. Get the soup\n",
    "def get_the_soup(url):\n",
    "    '''\n",
    "    Requests the URL and cooks it into a soup object. \n",
    "    ---\n",
    "    Arguments: url\n",
    "    Returns: soup\n",
    "    '''\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html=response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser') # parse html of web page\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ballotpedia.org/Alex_Padilla\"\n",
    "soup=get_the_soup(url)\n",
    "soup.prettify()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#2. Extract info from the soup\n",
    "from bs4 import NavigableString, Tag\n",
    "\n",
    "def extract_soup_info_to_dictionary(soup):\n",
    "    '''\n",
    "    Extracts relevant information from a bs4 object and stores it into a dictionary by html header. \n",
    "    ---\n",
    "    Arguments: soup\n",
    "    Returns: dictionary of text (value) by header (key)\n",
    "    '''\n",
    "    \n",
    "    dic_text_by_header=dict()\n",
    "    \n",
    "    # get all the text content between 2 header (h2)\n",
    "    for header in soup.find_all('h2')[0:len(soup.find_all('h2'))-1] :\n",
    "        # print('--------',header.get_text())        \n",
    "        nextNode=header\n",
    "        # use the nextSibling method\n",
    "        while True:\n",
    "            nextNode=nextNode.nextSibling\n",
    "            if nextNode is None:\n",
    "                break\n",
    "            if isinstance(nextNode, Tag):\n",
    "                if nextNode.name == \"h2\":\n",
    "                    break\n",
    "                #print(nextNode.get_text(strip=True).strip())\n",
    "                # The result is put in a dictionary as a value for key=corresponding header\n",
    "                dic_text_by_header[header.get_text()]=[nextNode.get_text(strip=True).strip()]\n",
    "    \n",
    "    return dic_text_by_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_soup_info_to_dictionary(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Putting everything together\n",
    "\n",
    "- A dataframe to put things together: `df_parsed`\n",
    "\n",
    "\n",
    "- **LOOP**: `url` from `list_url`): \n",
    "\n",
    "    - `get_the_soup()`: \n",
    "        - `url`  -> `soup`\n",
    "\n",
    "    - `extract_soup_info_to_dictionary()`: \n",
    "        - `soup` -> `dic_text_by_header`\n",
    "        \n",
    "    - `pd.concat()`\n",
    "        - `dic_text_by_header` -> `df_parsed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `pd.concat()`? \n",
    "Powerfull method to combine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### `pd.concat(list-of-dataframes)` \n",
    "Vertical or horizontal concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=0) # axis = 0 -> along the index, the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=1) # axis = 1 -> ON the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Wrappers: LOOPING over `list_url` using our homemade functions\n",
    "\n",
    "Writing requests manually is tedious. Wrappers add a layer of abstraction that makes this job easier for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use this short list (but cf. our list extracted above )\n",
    "list_url= ['https://ballotpedia.org/Katie_Britt',\n",
    " 'https://ballotpedia.org/Tommy_Tuberville',\n",
    " 'https://ballotpedia.org/Lisa_Murkowski'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import NavigableString, Tag\n",
    "\n",
    "# the dataframe in which we will put the scraper's output\n",
    "df_parsed=pd.DataFrame()\n",
    "\n",
    "for url in list_url:\n",
    "    print('--------',url, '--------')\n",
    "    #1. Get the soup\n",
    "    soup = get_the_soup(url)\n",
    "    \n",
    "    #2. Extract info from the soup\n",
    "    dic_text_by_header=extract_soup_info_to_dictionary(soup)\n",
    "                \n",
    "    # put the dictionary into a dataframe\n",
    "    temp=pd.DataFrame.from_dict(dic_text_by_header)\n",
    "\n",
    "    # Concats the temporary dataframe with the global one\n",
    "    df_parsed=pd.concat([temp, df_parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving the `DataFrame` in a `pickle` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-warning\"></i>&nbsp;<code>pickle</code> format\n",
    "    <ul>\n",
    "        <li> Useful to store <code>python</code> objects \n",
    "        </li>\n",
    "        <li> Well integrated in  <code>pandas</code> (using <code>to_pickle</code> and <code>read_pickle</code>)\n",
    "        </li>\n",
    "        <li> When the object is not a pandas Dataframe, use the <code>pickle</code> package\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Managing path\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<i class=\"fa fa-warning\"></i>&nbsp;<code>os</code> package\n",
    "    <ul>\n",
    "        <li> <code>os.getcwd()</code>: fetchs the current path\n",
    "        </li>\n",
    "        <li> <code>os.path.dirname()</code>: go back to the parent directory\n",
    "        </li>\n",
    "        <li> <code>os.path.join()</code>: concatenates several paths\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd() #   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parent_path=os.path.dirname(os.getcwd()) \n",
    "parent_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.path.join(parent_path, 'data') \n",
    "\n",
    "# Saving the data to pickle:\n",
    "df_parsed.to_pickle(os.path.join(data_path, 'df_senators.pickle'))\n",
    "\n",
    "# Saving the data to csv:\n",
    "df_parsed.to_csv(os.path.join(data_path, 'df_senators.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Going further\n",
    "\n",
    "There are also **dynamic websites**: the server does not always send back HTML, but your browser also receive and interpret JavaScript code that you cannot retreive from the HTML. You receive JavaScript code that you cannot parse using `beautiful soup` but that you would need to execute like a browser does. \n",
    "\n",
    "Solutions: \n",
    "- Use `requests-html` \n",
    "- Simulate a browser using [selenium](https://selenium-python.readthedocs.io/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application Programming Interfaces (API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is an API?\n",
    "<div class=\"r-stack\"><img src=\"https://raw.githubusercontent.com/malkaguillot/ECON2206-Data-Management/refs/heads/main/slides/images/api/API_3.svg?token=GHSAT0AAAAAACWYMATYOB76VG6756UFYU3UZYM5HYQ\" style=\"height: 360px;\" > </div>\n",
    "\n",
    "Communication layer that allows different systems to talk to each other without having to understand exactly what each other does.\n",
    "\n",
    "$\\Rightarrow$ provide a to **progammable** access to data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The (retired) website [Programmable Web](https://www.programmableweb.com/apis/directory) used to list more than 225,353 API from sites as diverse as Google, Amazon, YouTube, the New York Times, del.icio.us, LinkedIn, and many others.\n",
    "\n",
    "<center>\n",
    "<div class=\"r-stack\"><img src=\"https://www.researchgate.net/publication/362048443/figure/fig1/AS:11431281099021888@1669172071410/The-increasing-growth-of-Web-API-6.png\" style=\"height: 400px;\" > </div>\n",
    "</center>\n",
    "\n",
    "Source: [Programmable Web](https://www.programmableweb.com/news/apis-show-faster-growth-rate-2019-previous-years/research/2019/07/17) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Does an API Work?\n",
    "\n",
    "- Relying on **HTTP messages** :\n",
    "    - `request` for information or data, \n",
    "    - the API returns a `response` with what you requested\n",
    "- Similar to visiting a website: you specify a URL and information is sent to your machine.\n",
    "\n",
    "###  Better than webscraping if possible because: \n",
    "- More stable than webpages\n",
    "- No HTML but already structured data (e.g. in `json`)\n",
    "- we focus on the APIs that use HTTP protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><div class=\"r-stack\">\n",
    "<img src=\"https://s3.us-west-1.wasabisys.com/idbwmedia.com/images/api/restapi_restapi.svg\" style=\"height: 500px;\" > </div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Endpoints and Resources\n",
    "\n",
    "*An API endpoint is the end of your communication channel to the API. An API can have different endpoints for channels leading to different resources.*\n",
    "\n",
    "- **base URL**: https://api.carbonintensity.org.uk\n",
    "    - Other examples: https://api.twitter.com; https://api.github.com\n",
    "    - very basic information about an API, not the real data.\n",
    "- Extend the url with **endpoint**\n",
    "    - = a part of the URL that specifies what resource you want to fetch\n",
    "    - check the [documentation](https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0)\n",
    " to learn more about what endpoints are available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a request\n",
    "<div class=\"r-stack\"><img src=\"https://raw.githubusercontent.com/malkaguillot/ECON2206-Data-Management/refs/heads/main/slides/images/api/API_call_anatomy.svg?token=GHSAT0AAAAAACWYMATYD7WH4TQMCZNVL6EKZYM5NTA\" style=\"height: 160px;\" > </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of a response\n",
    "\n",
    "Very often API responses are formatted as JSON objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response = {\n",
    "    \"data\":\n",
    "    [\n",
    "        {\n",
    "            \"end\":\"2022-07-01\",\n",
    "            \"start\":\"2022-06-30\",\n",
    "            \"tweet_count\":3\n",
    "        },\n",
    "        {\n",
    "            \"end\":\"2022-07-02\",\n",
    "            \"start\":\"2022-07-01\",\n",
    "            \"tweet_count\":2\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "api_response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A json works as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response['data'][0]['end'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "Using the previous `api_response` json, extract the number of tweet for `2022-07-01`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_response['data'][1]['tweet_count'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HTTP Methods\n",
    "| HTTP Method | Description                  | Requests method   |\n",
    "|-------------|------------------------------|-------------------|\n",
    "| POST        | Create a new resource.       | requests.post()   |\n",
    "| GET         | Read an existing resource.   | requests.get()    |\n",
    "| PUT         | Update an existing resource. | requests.put()    |\n",
    "| DELETE      | Delete an existing resource. | requests.delete() |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calling Your First API Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasts from the [**Carbon Intensity API**](https://carbonintensity.org.uk/) (include CO2 emissions related to eletricity generation only).\n",
    "\n",
    "See the API [documentation](https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0)\n",
    "\n",
    "<center>\n",
    "<img src=\"attachment:image.png\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = { \n",
    "  'Accept': 'application/json'\n",
    "}\n",
    "# fetch (or get) data from the URL\n",
    "requests.get('https://api.carbonintensity.org.uk', params={}, headers = headers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.carbonintensity.org.uk', params={}, headers = headers) \n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Using the `intensity`  endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity data for current half hour\n",
    "r = requests.get('https://api.carbonintensity.org.uk/intensity', params={}, headers = headers)\n",
    "\n",
    "# Different outputs (same information):\n",
    "print(\"--- text ---\")\n",
    "pprint(r.text)\n",
    "print(\"--- Content ---\")\n",
    "pprint(r.content)\n",
    "print(\"--- JSON---\")\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h3><i class=\"fa fa-warning\"></i><code>json</code> </h3>\n",
    "    <ul>\n",
    "        <li><code>json</code>= python dictionary\n",
    "        </li>\n",
    "        <li>A great format for structured data\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json objects work as do any other dictionary in Python\n",
    "json=r.json()\n",
    "json['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the actual intensity value:\n",
    "json['data'][0]['intensity']['actual']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<h3> Your turn</h3>\n",
    "        <li>Get Carbon Intensity factors for each fuel type -> look for the relevant endpoint\n",
    "        </li>\n",
    "        <li> Get Carbon Intensity data for current half hour for GB regions\n",
    "        </li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://api.carbonintensity.org.uk/intensity/factors', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://api.carbonintensity.org.uk/regional', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.carbonintensity.org.uk/intensity/factors', params={}, headers = headers)\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Carbon Intensity data for current half hour for GB regions\n",
    "r = requests.get('https://api.carbonintensity.org.uk/regional', params={}, headers = headers)\n",
    "#pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Query Parameters\n",
    "- cf. slide on `url`\n",
    "- used as filters you can send with your API request to further narrow down the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the carbonintensity API, it works differently:\n",
    "from_=\"2018-08-25T12:35Z\"\n",
    "to=\"2018-08-25T13:35Z\"\n",
    "r = requests.get('https://api.carbonintensity.org.uk/regional/intensity/{}/{}'.format(from_, to), params={}, headers = headers)\n",
    "#pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### API Limitations\n",
    "To prevent collection of huge amount of individual data, many APIs require you to obtain “credentials” or codes/passwords that identify you and determine which types of data you are allowed to access. \n",
    "\n",
    "#### API Credentials\n",
    "- Different methods/level of authentification exist\n",
    "    - API keys\n",
    "    - OAuth      \n",
    "####  Rate Limits & quotas\n",
    "- The credentials also define how often we are allowed to make requests for data. \n",
    "- Be careful not to exceed the limits set by the API developers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### API Keys\n",
    "\n",
    "- Most common level of authentication \n",
    "- These keys are used to identify you as an API user or customer and to trace your use of the API. \n",
    "- API keys are typically sent as a request header or as a query parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples in research\n",
    "\n",
    "#### The Telegram API\n",
    "\n",
    "- *What do you get?* \n",
    "  - Messages & media in a given channel, channel members.\n",
    "- *Usefulness*: \n",
    "  - Very useful to get data from fringe-groups that are not represented on big social media platforms. Severly underused in computational social science research.\n",
    "- *Accessibility*: \n",
    "  - Clunky authentification process, lacking documentation, python wrapper not really designed for data collection.\n",
    "- *Example publication*: \n",
    "  - [Organization and evolution of the UK far-right network on Telegram](https://appliednetsci.springeropen.com/articles/10.1007/s41109-022-00513-8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The New York Times API\n",
    "- *What do you get?* \n",
    "  - Article abstracts and metadata, not full article texts.\n",
    "- *Usefulness*: \n",
    "  - Somewhat limited because full texts are missing, but abstracts can serve as a window into histporic high-quality journalistic texts.\n",
    "- *Accessibility*: \n",
    "  - Easy authentification, good documentation, nice python wrapper, hard to get article texts.\n",
    "- *Example publication*: \n",
    "  - [The rise and fall of rationality in language](https://www.pnas.org/doi/epdf/10.1073/pnas.2107848118) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other useful APIs:\n",
    "- Spotify   \n",
    "  - Metadata about artists, playlists & tracks, song features like \"danceability\".\n",
    "- CrossRef\n",
    "  - Article titles, authors, journals, number of references.\n",
    "- Open Street Maps\n",
    "  - Map data (nodes, ways, relations), alternative to Google maps.\n",
    "- The MediaWiki API \n",
    "  - Access wikipedia articles, article discussions & revision histories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General remarks\n",
    "- Start simple\n",
    "    - Expand your program incrementally.\n",
    "- Keep it simple. \n",
    "    - Do not overengineer the problem.\n",
    "- Do not repeat yourself. \n",
    "    - Code duplication implies bug reuse.\n",
    "- Limit the number of iterations for test runs. \n",
    "    - Use print statements toinspect objects.\n",
    "- Write tests to verify things work as intended.\n",
    "- If the web page cannot be navigated easily or has hidden javascript, look into Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More resources\n",
    "Excellent [tutorial](https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/) for a workflow from HTML to pandas DataFrame.\n",
    "\n",
    "- Web scraping with Selenium [Notebook]()\n",
    "- Web scraping [code snippets](https://github.com/JanaLasser/SICSS-aachen-graz/blob/main/02_01_APIs/exercise/web_scraping_code_snippets.ipynb).\n",
    "- API access [code snippets](https://github.com/JanaLasser/SICSS-aachen-graz/blob/main/02_01_APIs/exercise/API_access_code_snippets.ipynb).\n",
    "- Crowd-sourced [list](https://docs.google.com/spreadsheets/d/1ZEr3okdlb0zctmX0MZKo-gZKPsq5WGn1nJOxPV7al-Q/edit?gid=0#gid=0) of useful APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "DataManagement2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.7167px",
    "left": "1760.67px",
    "top": "0px",
    "width": "159.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "chalkboard": {
    "color": "rgb(220, 133, 41)"
   },
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
